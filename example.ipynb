{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jebeom/miniconda3/envs/lgdh/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jebeom/LGDH/denoisingheat/models/ddpm/denoising_diffusion.py:846: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from denoisingheat.utils.utils import (\n",
    "    gen_goals, overlay_goal, randgen_obstacle_masks, randgen_fix_obstacle_masks, draw_obstacles_pixel, load_config, overlay_goals_with_prompts, convert_to_obstacle_masks, draw_obstacles_pil\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "import shutil\n",
    "\n",
    "# Args\n",
    "config_dir = \"./heat_diffusion.yaml\"\n",
    "args = load_config(config_dir)\n",
    "device = args['device']\n",
    "\n",
    "bg = Image.open('assets/toy_exp/image.png') # need to change \n",
    "\n",
    "\n",
    "img_size = 96\n",
    "goal_bounds = args['goal_bounds']\n",
    "agent_bounds = args['agent_bounds']\n",
    "obstacle_pos = args['drop_region']\n",
    "\n",
    "model_path = os.path.join(args['log_path'], args['model_path'])\n",
    "\n",
    "u0 = args['u0']\n",
    "min_heat_step = args['min_heat_step']\n",
    "max_heat_step = args['max_heat_step']\n",
    "noise_steps = 20\n",
    "sample_num = 3\n",
    "time_type = args['time_type']\n",
    "\n",
    "iterations = args['iterations']\n",
    "train_lr = args['train_lr']\n",
    "batch_size = 5\n",
    "\n",
    "goal_num = 3\n",
    "\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def get_text_embeds(texts: list):\n",
    "    # texts: [\"apple\", \"banana\", ...]\n",
    "    # return: (B, seq_len, hidden_dim)\n",
    "    tokens = tokenizer(texts, padding=\"max_length\", max_length=10, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = text_encoder(**tokens)\n",
    "\n",
    "    text_embeds = outputs.last_hidden_state\n",
    "    return text_embeds\n",
    "\n",
    "# need to change \n",
    "goal_image_paths = [\n",
    "    'assets/toy_exp/apple.png',\n",
    "    'assets/toy_exp/banana.png',\n",
    "    'assets/toy_exp/clock.png',\n",
    "    'assets/toy_exp/blue_cup.png',\n",
    "    'assets/toy_exp/trash_can.png',\n",
    "    'assets/toy_exp/garbage1.png',\n",
    "    'assets/toy_exp/apple.png'\n",
    "]\n",
    "\n",
    "goal_images = [Image.open(path) for path in goal_image_paths]\n",
    "original_images = goal_images.copy()\n",
    "base_size = original_images[0].size\n",
    "\n",
    "goal_images_resized = [img.resize(base_size, Image.LANCZOS) for img in goal_images]\n",
    "# need to change \n",
    "goal_prompts = [\n",
    "    \"Move to the apple.\",\n",
    "    \"Move to the banana.\",\n",
    "    \"Move to the clock.\",\n",
    "    \"Move to the blue cup.\",\n",
    "    \"Move to the trash can.\",\n",
    "    \"Move to the garbage.\",\n",
    "    \"Move to the apple.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sel_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mmultinomial(          \u001b[38;5;66;03m# (B, k)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     torch\u001b[38;5;241m.\u001b[39mones(batch_size, goal_num, device\u001b[38;5;241m=\u001b[39mdevice),  \n\u001b[1;32m      3\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      4\u001b[0m     replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m flat_sel   \u001b[38;5;241m=\u001b[39m sel_idx\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)                      \u001b[38;5;66;03m# (B*k,)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m batch_ids  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(batch_size, device\u001b[38;5;241m=\u001b[39mdevice) \\\n\u001b[1;32m      8\u001b[0m                 \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# (B*k,)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "sel_idx = torch.multinomial(          # (B, k)\n",
    "    torch.ones(batch_size, goal_num, device=device),  \n",
    "    num_samples=1,\n",
    "    replacement=False\n",
    ")\n",
    "flat_sel   = sel_idx.reshape(-1)                      # (B*k,)\n",
    "batch_ids  = torch.arange(batch_size, device=device) \\\n",
    "                .unsqueeze(1).expand(-1, 1).reshape(-1)   # (B*k,)\n",
    "\n",
    "# need to change to real obstacle mask\n",
    "obstacle_masks = randgen_obstacle_masks(batch_size*1, img_size, device=device)\n",
    "background = draw_obstacles_pixel(bg, obstacle_masks)\n",
    "multi_goals = gen_goals(goal_bounds, (goal_num, batch_size), img_size, obstacles=obstacle_masks[:], device=device)\n",
    "multi_goal = (torch.rand(batch_size, goal_num, 2, device=device, dtype=torch.float32) * 0.2 - 0.1) * 0.05 + multi_goals\n",
    "# need to change ?\n",
    "obs1, prompts1 = overlay_goals_with_prompts(background, img_size, goal_images_resized, multi_goal, goal_prompts, goal_size=16)\n",
    "obs  = obs1[batch_ids]\n",
    "\n",
    "print(np.shape(obs))\n",
    "plt.imshow(obs.permute(0,2,3,1)[0].cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(obs.permute(0,2,3,1)[1].cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(obs.permute(0,2,3,1)[2].cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(obs.permute(0,2,3,1)[3].cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(obs.permute(0,2,3,1)[4].cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(obs.permute(0,2,3,1)[5].cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(obs.permute(0,2,3,1)[6].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgdh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
